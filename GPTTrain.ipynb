{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44d9d764fcd45d598efc48baf1bac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d36e8375954d94a80546c594e62fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22efdfadccd4f3e8bed6ee4b6de1f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d5e94b492a437597cbc5ae29de4645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55441c5a98b04fc3913b7125ab26edc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b2886617b1422ea4815821b55257fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/cola to /home/featurize/.cache/huggingface/datasets/mariosasko___glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10221b844164187bf8e59fc4ff08db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/377k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/featurize/.cache/huggingface/datasets/mariosasko___glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8197fd2f92b54f3b93d6b70e7e657691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup, Adafactor\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from lion_pytorch import Lion\n",
    "from torch.utils.data import DataLoader\n",
    "import GLUEGPT\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_name = 'cola'\n",
    "current_path = Path.cwd().parents[0] / dataset_name\n",
    "current_path.mkdir(exist_ok=True)\n",
    "\n",
    "lr_list = [3e-4]\n",
    "scheduler_list = ['no']\n",
    "optimizer_list = ['AdamW']\n",
    "batch_size_list = [32]\n",
    "steps = 4 * 1000\n",
    "report_step = 50\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "train_dataset, _, test_dataset = GLUEGPT.get_torch_dataset(tokenizer, \"cola\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def constant_scheduler(\n",
    "    optimizer, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1\n",
    "):\n",
    "    def lambda_func(step: int):\n",
    "        return 1.\n",
    "\n",
    "    return LambdaLR(optimizer, lambda_func, last_epoch)\n",
    "\n",
    "\n",
    "def prepare(sche, opt):\n",
    "    if sche == 'no':\n",
    "        sches = partial(constant_scheduler)\n",
    "    if sche == 'linear':\n",
    "        sches = partial(get_linear_schedule_with_warmup)\n",
    "    if sche == 'ord10':\n",
    "        sches = partial(get_polynomial_decay_schedule_with_warmup, power=10.0)\n",
    "\n",
    "    if opt == 'Lion':\n",
    "        opts = partial(Lion, betas=(0.95, 0.98), weight_decay=0.01)\n",
    "    if opt == 'AdaFactor':\n",
    "        opts = partial(Adafactor, weight_decay=0.001, relative_step=False, scale_parameter=False)\n",
    "    if opt == 'AdamW':\n",
    "        opts = partial(torch.optim.AdamW, betas=(0.9, 0.99), weight_decay=0.001)\n",
    "\n",
    "    return sches, opts\n",
    "\n",
    "\n",
    "def get_log(file_name):\n",
    "    logger = logging.getLogger('train')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "\n",
    "    fh = logging.FileHandler(file_name, mode='a')\n",
    "    fh.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = get_log('log1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    eval_loader = DataLoader(dataset, shuffle=False, batch_size=32)\n",
    "    logits = []\n",
    "    labelss = []\n",
    "    with torch.no_grad():\n",
    "        for X in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in X.items()}\n",
    "            logits.append(model(**batch).logits)\n",
    "            labelss.append(batch['labels'])\n",
    "        total_test = torch.concat(logits, dim=0)\n",
    "        _, predicted = torch.max(total_test, dim=1)\n",
    "        real_label = torch.concat(labelss, dim=0).cpu().numpy()\n",
    "        predicted = predicted.cpu().numpy()\n",
    "        metric = matthews_corrcoef(real_label, predicted)\n",
    "        acc = np.mean(predicted == real_label)\n",
    "\n",
    "    return metric, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-03-17 22:01:04,102 - INFO - step:0, matthews_corr:0.000000, Acc:30.872483%, Train: matthews_corr:0.000000, Acc:29.563794%,\n",
      "INFO:train:step:0, matthews_corr:0.000000, Acc:30.872483%, Train: matthews_corr:0.000000, Acc:29.563794%,\n",
      "2023-03-17 22:01:04,185 - INFO - step: 1, loss:3.04088902\n",
      "INFO:train:step: 1, loss:3.04088902\n",
      "2023-03-17 22:01:04,412 - INFO - step: 2, loss:0.64065135\n",
      "INFO:train:step: 2, loss:0.64065135\n",
      "2023-03-17 22:01:04,627 - INFO - step: 3, loss:0.76807135\n",
      "INFO:train:step: 3, loss:0.76807135\n",
      "2023-03-17 22:01:04,841 - INFO - step: 4, loss:0.63765377\n",
      "INFO:train:step: 4, loss:0.63765377\n",
      "2023-03-17 22:01:05,055 - INFO - step: 5, loss:0.74461001\n",
      "INFO:train:step: 5, loss:0.74461001\n",
      "2023-03-17 22:01:05,270 - INFO - step: 6, loss:0.73536736\n",
      "INFO:train:step: 6, loss:0.73536736\n",
      "2023-03-17 22:01:05,483 - INFO - step: 7, loss:0.83121991\n",
      "INFO:train:step: 7, loss:0.83121991\n",
      "2023-03-17 22:01:05,698 - INFO - step: 8, loss:0.71665376\n",
      "INFO:train:step: 8, loss:0.71665376\n",
      "2023-03-17 22:01:05,912 - INFO - step: 9, loss:0.66541237\n",
      "INFO:train:step: 9, loss:0.66541237\n",
      "2023-03-17 22:01:06,126 - INFO - step: 10, loss:0.72197998\n",
      "INFO:train:step: 10, loss:0.72197998\n",
      "2023-03-17 22:01:06,340 - INFO - step: 11, loss:0.58102381\n",
      "INFO:train:step: 11, loss:0.58102381\n",
      "2023-03-17 22:01:06,554 - INFO - step: 12, loss:0.62618589\n",
      "INFO:train:step: 12, loss:0.62618589\n",
      "2023-03-17 22:01:06,768 - INFO - step: 13, loss:0.69029975\n",
      "INFO:train:step: 13, loss:0.69029975\n",
      "2023-03-17 22:01:06,982 - INFO - step: 14, loss:0.75031078\n",
      "INFO:train:step: 14, loss:0.75031078\n",
      "2023-03-17 22:01:07,196 - INFO - step: 15, loss:0.58508104\n",
      "INFO:train:step: 15, loss:0.58508104\n",
      "2023-03-17 22:01:07,412 - INFO - step: 16, loss:0.60356510\n",
      "INFO:train:step: 16, loss:0.60356510\n",
      "2023-03-17 22:01:07,627 - INFO - step: 17, loss:0.64078605\n",
      "INFO:train:step: 17, loss:0.64078605\n",
      "2023-03-17 22:01:07,843 - INFO - step: 18, loss:0.39664748\n",
      "INFO:train:step: 18, loss:0.39664748\n",
      "2023-03-17 22:01:08,057 - INFO - step: 19, loss:0.74995530\n",
      "INFO:train:step: 19, loss:0.74995530\n",
      "2023-03-17 22:01:08,270 - INFO - step: 20, loss:0.51231635\n",
      "INFO:train:step: 20, loss:0.51231635\n",
      "2023-03-17 22:01:08,483 - INFO - step: 21, loss:0.52310771\n",
      "INFO:train:step: 21, loss:0.52310771\n",
      "2023-03-17 22:01:08,696 - INFO - step: 22, loss:0.62535590\n",
      "INFO:train:step: 22, loss:0.62535590\n",
      "2023-03-17 22:01:08,910 - INFO - step: 23, loss:0.87124991\n",
      "INFO:train:step: 23, loss:0.87124991\n",
      "2023-03-17 22:01:09,123 - INFO - step: 24, loss:0.83392364\n",
      "INFO:train:step: 24, loss:0.83392364\n",
      "2023-03-17 22:01:09,337 - INFO - step: 25, loss:0.67162687\n",
      "INFO:train:step: 25, loss:0.67162687\n",
      "2023-03-17 22:01:09,550 - INFO - step: 26, loss:0.71202862\n",
      "INFO:train:step: 26, loss:0.71202862\n",
      "2023-03-17 22:01:09,763 - INFO - step: 27, loss:0.69860816\n",
      "INFO:train:step: 27, loss:0.69860816\n",
      "2023-03-17 22:01:09,977 - INFO - step: 28, loss:0.71862447\n",
      "INFO:train:step: 28, loss:0.71862447\n",
      "2023-03-17 22:01:10,189 - INFO - step: 29, loss:0.69755131\n",
      "INFO:train:step: 29, loss:0.69755131\n",
      "2023-03-17 22:01:10,402 - INFO - step: 30, loss:0.75685126\n",
      "INFO:train:step: 30, loss:0.75685126\n",
      "2023-03-17 22:01:10,615 - INFO - step: 31, loss:0.71117812\n",
      "INFO:train:step: 31, loss:0.71117812\n",
      "2023-03-17 22:01:10,828 - INFO - step: 32, loss:0.77377701\n",
      "INFO:train:step: 32, loss:0.77377701\n",
      "2023-03-17 22:01:11,042 - INFO - step: 33, loss:0.67532223\n",
      "INFO:train:step: 33, loss:0.67532223\n",
      "2023-03-17 22:01:11,255 - INFO - step: 34, loss:0.62820405\n",
      "INFO:train:step: 34, loss:0.62820405\n",
      "2023-03-17 22:01:11,468 - INFO - step: 35, loss:0.63892829\n",
      "INFO:train:step: 35, loss:0.63892829\n",
      "2023-03-17 22:01:11,681 - INFO - step: 36, loss:0.72773886\n",
      "INFO:train:step: 36, loss:0.72773886\n",
      "2023-03-17 22:01:11,894 - INFO - step: 37, loss:0.73707330\n",
      "INFO:train:step: 37, loss:0.73707330\n",
      "2023-03-17 22:01:12,107 - INFO - step: 38, loss:0.62250495\n",
      "INFO:train:step: 38, loss:0.62250495\n",
      "2023-03-17 22:01:12,320 - INFO - step: 39, loss:0.56953096\n",
      "INFO:train:step: 39, loss:0.56953096\n",
      "2023-03-17 22:01:12,532 - INFO - step: 40, loss:0.81388575\n",
      "INFO:train:step: 40, loss:0.81388575\n",
      "2023-03-17 22:01:12,745 - INFO - step: 41, loss:0.76494694\n",
      "INFO:train:step: 41, loss:0.76494694\n",
      "2023-03-17 22:01:12,958 - INFO - step: 42, loss:0.66147852\n",
      "INFO:train:step: 42, loss:0.66147852\n",
      "2023-03-17 22:01:13,171 - INFO - step: 43, loss:0.80301100\n",
      "INFO:train:step: 43, loss:0.80301100\n",
      "2023-03-17 22:01:13,384 - INFO - step: 44, loss:0.70639622\n",
      "INFO:train:step: 44, loss:0.70639622\n",
      "2023-03-17 22:01:13,597 - INFO - step: 45, loss:0.64243639\n",
      "INFO:train:step: 45, loss:0.64243639\n",
      "2023-03-17 22:01:13,810 - INFO - step: 46, loss:0.71391064\n",
      "INFO:train:step: 46, loss:0.71391064\n",
      "2023-03-17 22:01:14,023 - INFO - step: 47, loss:0.55212981\n",
      "INFO:train:step: 47, loss:0.55212981\n",
      "2023-03-17 22:01:14,236 - INFO - step: 48, loss:0.63239229\n",
      "INFO:train:step: 48, loss:0.63239229\n",
      "2023-03-17 22:01:14,449 - INFO - step: 49, loss:0.63731396\n",
      "INFO:train:step: 49, loss:0.63731396\n",
      "2023-03-17 22:01:14,662 - INFO - step: 50, loss:0.63754618\n",
      "INFO:train:step: 50, loss:0.63754618\n",
      "2023-03-17 22:01:35,672 - INFO - step:50, matthews_corr:0.050264, Acc:68.552253%, Train: matthews_corr:0.034817, Acc:69.266752%,\n",
      "INFO:train:step:50, matthews_corr:0.050264, Acc:68.552253%, Train: matthews_corr:0.034817, Acc:69.266752%,\n",
      "2023-03-17 22:01:35,750 - INFO - step: 51, loss:0.63034517\n",
      "INFO:train:step: 51, loss:0.63034517\n",
      "2023-03-17 22:01:35,964 - INFO - step: 52, loss:0.76226062\n",
      "INFO:train:step: 52, loss:0.76226062\n",
      "2023-03-17 22:01:36,177 - INFO - step: 53, loss:0.71384019\n",
      "INFO:train:step: 53, loss:0.71384019\n",
      "2023-03-17 22:01:36,391 - INFO - step: 54, loss:0.73832130\n",
      "INFO:train:step: 54, loss:0.73832130\n",
      "2023-03-17 22:01:36,604 - INFO - step: 55, loss:0.66052431\n",
      "INFO:train:step: 55, loss:0.66052431\n",
      "2023-03-17 22:01:36,818 - INFO - step: 56, loss:0.65638095\n",
      "INFO:train:step: 56, loss:0.65638095\n",
      "2023-03-17 22:01:37,031 - INFO - step: 57, loss:0.68312776\n",
      "INFO:train:step: 57, loss:0.68312776\n",
      "2023-03-17 22:01:37,245 - INFO - step: 58, loss:0.71372044\n",
      "INFO:train:step: 58, loss:0.71372044\n",
      "2023-03-17 22:01:37,458 - INFO - step: 59, loss:0.68928075\n",
      "INFO:train:step: 59, loss:0.68928075\n",
      "2023-03-17 22:01:37,672 - INFO - step: 60, loss:0.67886025\n",
      "INFO:train:step: 60, loss:0.67886025\n",
      "2023-03-17 22:01:37,886 - INFO - step: 61, loss:0.67304254\n",
      "INFO:train:step: 61, loss:0.67304254\n",
      "2023-03-17 22:01:38,099 - INFO - step: 62, loss:0.68616307\n",
      "INFO:train:step: 62, loss:0.68616307\n",
      "2023-03-17 22:01:38,313 - INFO - step: 63, loss:0.60215318\n",
      "INFO:train:step: 63, loss:0.60215318\n",
      "2023-03-17 22:01:38,526 - INFO - step: 64, loss:0.71278524\n",
      "INFO:train:step: 64, loss:0.71278524\n",
      "2023-03-17 22:01:38,740 - INFO - step: 65, loss:0.65306026\n",
      "INFO:train:step: 65, loss:0.65306026\n",
      "2023-03-17 22:01:38,953 - INFO - step: 66, loss:0.66365135\n",
      "INFO:train:step: 66, loss:0.66365135\n",
      "2023-03-17 22:01:39,167 - INFO - step: 67, loss:0.55069679\n",
      "INFO:train:step: 67, loss:0.55069679\n",
      "2023-03-17 22:01:39,381 - INFO - step: 68, loss:0.62325335\n",
      "INFO:train:step: 68, loss:0.62325335\n",
      "2023-03-17 22:01:39,594 - INFO - step: 69, loss:0.63381332\n",
      "INFO:train:step: 69, loss:0.63381332\n",
      "2023-03-17 22:01:39,808 - INFO - step: 70, loss:0.47822931\n",
      "INFO:train:step: 70, loss:0.47822931\n",
      "2023-03-17 22:01:40,021 - INFO - step: 71, loss:0.49783456\n",
      "INFO:train:step: 71, loss:0.49783456\n",
      "2023-03-17 22:01:40,235 - INFO - step: 72, loss:0.67892176\n",
      "INFO:train:step: 72, loss:0.67892176\n",
      "2023-03-17 22:01:40,448 - INFO - step: 73, loss:0.68085706\n",
      "INFO:train:step: 73, loss:0.68085706\n",
      "2023-03-17 22:01:40,661 - INFO - step: 74, loss:0.69266027\n",
      "INFO:train:step: 74, loss:0.69266027\n",
      "2023-03-17 22:01:40,875 - INFO - step: 75, loss:0.48878646\n",
      "INFO:train:step: 75, loss:0.48878646\n",
      "2023-03-17 22:01:41,088 - INFO - step: 76, loss:0.39263675\n",
      "INFO:train:step: 76, loss:0.39263675\n",
      "2023-03-17 22:01:41,301 - INFO - step: 77, loss:0.42941681\n",
      "INFO:train:step: 77, loss:0.42941681\n",
      "2023-03-17 22:01:41,515 - INFO - step: 78, loss:0.53984714\n",
      "INFO:train:step: 78, loss:0.53984714\n",
      "2023-03-17 22:01:41,728 - INFO - step: 79, loss:0.71424592\n",
      "INFO:train:step: 79, loss:0.71424592\n",
      "2023-03-17 22:01:41,942 - INFO - step: 80, loss:1.06191862\n",
      "INFO:train:step: 80, loss:1.06191862\n",
      "2023-03-17 22:01:42,155 - INFO - step: 81, loss:0.79992878\n",
      "INFO:train:step: 81, loss:0.79992878\n",
      "2023-03-17 22:01:42,369 - INFO - step: 82, loss:0.84153563\n",
      "INFO:train:step: 82, loss:0.84153563\n",
      "2023-03-17 22:01:42,582 - INFO - step: 83, loss:0.56009245\n",
      "INFO:train:step: 83, loss:0.56009245\n",
      "2023-03-17 22:01:42,796 - INFO - step: 84, loss:0.71442413\n",
      "INFO:train:step: 84, loss:0.71442413\n",
      "2023-03-17 22:01:43,009 - INFO - step: 85, loss:0.62322676\n",
      "INFO:train:step: 85, loss:0.62322676\n",
      "2023-03-17 22:01:43,222 - INFO - step: 86, loss:0.76039243\n",
      "INFO:train:step: 86, loss:0.76039243\n",
      "2023-03-17 22:01:43,436 - INFO - step: 87, loss:0.67567891\n",
      "INFO:train:step: 87, loss:0.67567891\n",
      "2023-03-17 22:01:43,648 - INFO - step: 88, loss:0.70745873\n",
      "INFO:train:step: 88, loss:0.70745873\n",
      "2023-03-17 22:01:43,861 - INFO - step: 89, loss:0.69143748\n",
      "INFO:train:step: 89, loss:0.69143748\n",
      "2023-03-17 22:01:44,073 - INFO - step: 90, loss:0.75217766\n",
      "INFO:train:step: 90, loss:0.75217766\n",
      "2023-03-17 22:01:44,285 - INFO - step: 91, loss:0.73430222\n",
      "INFO:train:step: 91, loss:0.73430222\n",
      "2023-03-17 22:01:44,498 - INFO - step: 92, loss:0.73811150\n",
      "INFO:train:step: 92, loss:0.73811150\n",
      "2023-03-17 22:01:44,710 - INFO - step: 93, loss:0.68975931\n",
      "INFO:train:step: 93, loss:0.68975931\n",
      "2023-03-17 22:01:44,923 - INFO - step: 94, loss:0.65865082\n",
      "INFO:train:step: 94, loss:0.65865082\n",
      "2023-03-17 22:01:45,135 - INFO - step: 95, loss:0.66751784\n",
      "INFO:train:step: 95, loss:0.66751784\n",
      "2023-03-17 22:01:45,348 - INFO - step: 96, loss:0.55288202\n",
      "INFO:train:step: 96, loss:0.55288202\n",
      "2023-03-17 22:01:45,560 - INFO - step: 97, loss:0.59617090\n",
      "INFO:train:step: 97, loss:0.59617090\n",
      "2023-03-17 22:01:45,773 - INFO - step: 98, loss:0.81134105\n",
      "INFO:train:step: 98, loss:0.81134105\n",
      "2023-03-17 22:01:45,985 - INFO - step: 99, loss:0.60173935\n",
      "INFO:train:step: 99, loss:0.60173935\n",
      "2023-03-17 22:01:46,198 - INFO - step: 100, loss:0.73558277\n",
      "INFO:train:step: 100, loss:0.73558277\n",
      "2023-03-17 22:02:07,575 - INFO - step:100, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:100, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:02:07,657 - INFO - step: 101, loss:0.59792399\n",
      "INFO:train:step: 101, loss:0.59792399\n",
      "2023-03-17 22:02:07,871 - INFO - step: 102, loss:0.56891394\n",
      "INFO:train:step: 102, loss:0.56891394\n",
      "2023-03-17 22:02:08,085 - INFO - step: 103, loss:0.77979064\n",
      "INFO:train:step: 103, loss:0.77979064\n",
      "2023-03-17 22:02:08,300 - INFO - step: 104, loss:0.46686172\n",
      "INFO:train:step: 104, loss:0.46686172\n",
      "2023-03-17 22:02:08,514 - INFO - step: 105, loss:0.45379162\n",
      "INFO:train:step: 105, loss:0.45379162\n",
      "2023-03-17 22:02:08,728 - INFO - step: 106, loss:0.44276261\n",
      "INFO:train:step: 106, loss:0.44276261\n",
      "2023-03-17 22:02:08,942 - INFO - step: 107, loss:0.68174815\n",
      "INFO:train:step: 107, loss:0.68174815\n",
      "2023-03-17 22:02:09,155 - INFO - step: 108, loss:0.59846592\n",
      "INFO:train:step: 108, loss:0.59846592\n",
      "2023-03-17 22:02:09,369 - INFO - step: 109, loss:0.86532879\n",
      "INFO:train:step: 109, loss:0.86532879\n",
      "2023-03-17 22:02:09,583 - INFO - step: 110, loss:0.52972203\n",
      "INFO:train:step: 110, loss:0.52972203\n",
      "2023-03-17 22:02:09,797 - INFO - step: 111, loss:0.65079761\n",
      "INFO:train:step: 111, loss:0.65079761\n",
      "2023-03-17 22:02:10,010 - INFO - step: 112, loss:0.62295669\n",
      "INFO:train:step: 112, loss:0.62295669\n",
      "2023-03-17 22:02:10,224 - INFO - step: 113, loss:0.50522327\n",
      "INFO:train:step: 113, loss:0.50522327\n",
      "2023-03-17 22:02:10,438 - INFO - step: 114, loss:0.57328385\n",
      "INFO:train:step: 114, loss:0.57328385\n",
      "2023-03-17 22:02:10,652 - INFO - step: 115, loss:0.65196800\n",
      "INFO:train:step: 115, loss:0.65196800\n",
      "2023-03-17 22:02:10,866 - INFO - step: 116, loss:0.45379525\n",
      "INFO:train:step: 116, loss:0.45379525\n",
      "2023-03-17 22:02:11,080 - INFO - step: 117, loss:0.51741993\n",
      "INFO:train:step: 117, loss:0.51741993\n",
      "2023-03-17 22:02:11,294 - INFO - step: 118, loss:0.48842180\n",
      "INFO:train:step: 118, loss:0.48842180\n",
      "2023-03-17 22:02:11,508 - INFO - step: 119, loss:0.39211121\n",
      "INFO:train:step: 119, loss:0.39211121\n",
      "2023-03-17 22:02:11,722 - INFO - step: 120, loss:0.43892455\n",
      "INFO:train:step: 120, loss:0.43892455\n",
      "2023-03-17 22:02:11,936 - INFO - step: 121, loss:0.46572980\n",
      "INFO:train:step: 121, loss:0.46572980\n",
      "2023-03-17 22:02:12,150 - INFO - step: 122, loss:0.74614412\n",
      "INFO:train:step: 122, loss:0.74614412\n",
      "2023-03-17 22:02:12,363 - INFO - step: 123, loss:0.77082670\n",
      "INFO:train:step: 123, loss:0.77082670\n",
      "2023-03-17 22:02:12,577 - INFO - step: 124, loss:0.32164559\n",
      "INFO:train:step: 124, loss:0.32164559\n",
      "2023-03-17 22:02:12,791 - INFO - step: 125, loss:0.72696799\n",
      "INFO:train:step: 125, loss:0.72696799\n",
      "2023-03-17 22:02:13,005 - INFO - step: 126, loss:0.82755572\n",
      "INFO:train:step: 126, loss:0.82755572\n",
      "2023-03-17 22:02:13,219 - INFO - step: 127, loss:0.52299857\n",
      "INFO:train:step: 127, loss:0.52299857\n",
      "2023-03-17 22:02:13,433 - INFO - step: 128, loss:0.48880026\n",
      "INFO:train:step: 128, loss:0.48880026\n",
      "2023-03-17 22:02:13,647 - INFO - step: 129, loss:0.89248806\n",
      "INFO:train:step: 129, loss:0.89248806\n",
      "2023-03-17 22:02:13,861 - INFO - step: 130, loss:0.63168561\n",
      "INFO:train:step: 130, loss:0.63168561\n",
      "2023-03-17 22:02:14,074 - INFO - step: 131, loss:0.52563357\n",
      "INFO:train:step: 131, loss:0.52563357\n",
      "2023-03-17 22:02:14,288 - INFO - step: 132, loss:0.62609357\n",
      "INFO:train:step: 132, loss:0.62609357\n",
      "2023-03-17 22:02:14,502 - INFO - step: 133, loss:0.59158635\n",
      "INFO:train:step: 133, loss:0.59158635\n",
      "2023-03-17 22:02:14,716 - INFO - step: 134, loss:0.70581514\n",
      "INFO:train:step: 134, loss:0.70581514\n",
      "2023-03-17 22:02:14,929 - INFO - step: 135, loss:0.62738997\n",
      "INFO:train:step: 135, loss:0.62738997\n",
      "2023-03-17 22:02:15,143 - INFO - step: 136, loss:0.81487924\n",
      "INFO:train:step: 136, loss:0.81487924\n",
      "2023-03-17 22:02:15,357 - INFO - step: 137, loss:0.67340755\n",
      "INFO:train:step: 137, loss:0.67340755\n",
      "2023-03-17 22:02:15,571 - INFO - step: 138, loss:0.72341967\n",
      "INFO:train:step: 138, loss:0.72341967\n",
      "2023-03-17 22:02:15,784 - INFO - step: 139, loss:0.60249031\n",
      "INFO:train:step: 139, loss:0.60249031\n",
      "2023-03-17 22:02:15,998 - INFO - step: 140, loss:0.72483528\n",
      "INFO:train:step: 140, loss:0.72483528\n",
      "2023-03-17 22:02:16,212 - INFO - step: 141, loss:0.66266215\n",
      "INFO:train:step: 141, loss:0.66266215\n",
      "2023-03-17 22:02:16,426 - INFO - step: 142, loss:0.64280599\n",
      "INFO:train:step: 142, loss:0.64280599\n",
      "2023-03-17 22:02:16,640 - INFO - step: 143, loss:0.70695984\n",
      "INFO:train:step: 143, loss:0.70695984\n",
      "2023-03-17 22:02:16,854 - INFO - step: 144, loss:0.63814342\n",
      "INFO:train:step: 144, loss:0.63814342\n",
      "2023-03-17 22:02:17,068 - INFO - step: 145, loss:0.65674704\n",
      "INFO:train:step: 145, loss:0.65674704\n",
      "2023-03-17 22:02:17,281 - INFO - step: 146, loss:0.63463342\n",
      "INFO:train:step: 146, loss:0.63463342\n",
      "2023-03-17 22:02:17,495 - INFO - step: 147, loss:0.62197191\n",
      "INFO:train:step: 147, loss:0.62197191\n",
      "2023-03-17 22:02:17,711 - INFO - step: 148, loss:0.60989666\n",
      "INFO:train:step: 148, loss:0.60989666\n",
      "2023-03-17 22:02:17,926 - INFO - step: 149, loss:0.61287779\n",
      "INFO:train:step: 149, loss:0.61287779\n",
      "2023-03-17 22:02:18,140 - INFO - step: 150, loss:0.53372031\n",
      "INFO:train:step: 150, loss:0.53372031\n",
      "2023-03-17 22:02:39,059 - INFO - step:150, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:150, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:02:39,140 - INFO - step: 151, loss:0.65601027\n",
      "INFO:train:step: 151, loss:0.65601027\n",
      "2023-03-17 22:02:39,354 - INFO - step: 152, loss:0.62457252\n",
      "INFO:train:step: 152, loss:0.62457252\n",
      "2023-03-17 22:02:39,568 - INFO - step: 153, loss:0.62226313\n",
      "INFO:train:step: 153, loss:0.62226313\n",
      "2023-03-17 22:02:39,782 - INFO - step: 154, loss:0.59519517\n",
      "INFO:train:step: 154, loss:0.59519517\n",
      "2023-03-17 22:02:39,996 - INFO - step: 155, loss:0.60274625\n",
      "INFO:train:step: 155, loss:0.60274625\n",
      "2023-03-17 22:02:40,210 - INFO - step: 156, loss:0.69429266\n",
      "INFO:train:step: 156, loss:0.69429266\n",
      "2023-03-17 22:02:40,425 - INFO - step: 157, loss:0.56039536\n",
      "INFO:train:step: 157, loss:0.56039536\n",
      "2023-03-17 22:02:40,640 - INFO - step: 158, loss:0.41041300\n",
      "INFO:train:step: 158, loss:0.41041300\n",
      "2023-03-17 22:02:40,853 - INFO - step: 159, loss:0.44362080\n",
      "INFO:train:step: 159, loss:0.44362080\n",
      "2023-03-17 22:02:41,067 - INFO - step: 160, loss:0.69276643\n",
      "INFO:train:step: 160, loss:0.69276643\n",
      "2023-03-17 22:02:41,280 - INFO - step: 161, loss:0.64772326\n",
      "INFO:train:step: 161, loss:0.64772326\n",
      "2023-03-17 22:02:41,493 - INFO - step: 162, loss:0.44880956\n",
      "INFO:train:step: 162, loss:0.44880956\n",
      "2023-03-17 22:02:41,706 - INFO - step: 163, loss:0.36160386\n",
      "INFO:train:step: 163, loss:0.36160386\n",
      "2023-03-17 22:02:41,920 - INFO - step: 164, loss:0.57001686\n",
      "INFO:train:step: 164, loss:0.57001686\n",
      "2023-03-17 22:02:42,134 - INFO - step: 165, loss:0.57233840\n",
      "INFO:train:step: 165, loss:0.57233840\n",
      "2023-03-17 22:02:42,348 - INFO - step: 166, loss:0.79416400\n",
      "INFO:train:step: 166, loss:0.79416400\n",
      "2023-03-17 22:02:42,562 - INFO - step: 167, loss:0.57655400\n",
      "INFO:train:step: 167, loss:0.57655400\n",
      "2023-03-17 22:02:42,776 - INFO - step: 168, loss:0.56020951\n",
      "INFO:train:step: 168, loss:0.56020951\n",
      "2023-03-17 22:02:42,990 - INFO - step: 169, loss:0.72643334\n",
      "INFO:train:step: 169, loss:0.72643334\n",
      "2023-03-17 22:02:43,203 - INFO - step: 170, loss:0.78136003\n",
      "INFO:train:step: 170, loss:0.78136003\n",
      "2023-03-17 22:02:43,417 - INFO - step: 171, loss:0.72934288\n",
      "INFO:train:step: 171, loss:0.72934288\n",
      "2023-03-17 22:02:43,631 - INFO - step: 172, loss:0.47388878\n",
      "INFO:train:step: 172, loss:0.47388878\n",
      "2023-03-17 22:02:43,845 - INFO - step: 173, loss:0.67350286\n",
      "INFO:train:step: 173, loss:0.67350286\n",
      "2023-03-17 22:02:44,059 - INFO - step: 174, loss:0.67250896\n",
      "INFO:train:step: 174, loss:0.67250896\n",
      "2023-03-17 22:02:44,272 - INFO - step: 175, loss:0.57059085\n",
      "INFO:train:step: 175, loss:0.57059085\n",
      "2023-03-17 22:02:44,486 - INFO - step: 176, loss:0.57279128\n",
      "INFO:train:step: 176, loss:0.57279128\n",
      "2023-03-17 22:02:44,700 - INFO - step: 177, loss:0.56662673\n",
      "INFO:train:step: 177, loss:0.56662673\n",
      "2023-03-17 22:02:44,914 - INFO - step: 178, loss:0.47979495\n",
      "INFO:train:step: 178, loss:0.47979495\n",
      "2023-03-17 22:02:45,128 - INFO - step: 179, loss:0.48610705\n",
      "INFO:train:step: 179, loss:0.48610705\n",
      "2023-03-17 22:02:45,341 - INFO - step: 180, loss:0.42127612\n",
      "INFO:train:step: 180, loss:0.42127612\n",
      "2023-03-17 22:02:45,555 - INFO - step: 181, loss:0.47171152\n",
      "INFO:train:step: 181, loss:0.47171152\n",
      "2023-03-17 22:02:45,769 - INFO - step: 182, loss:0.47253352\n",
      "INFO:train:step: 182, loss:0.47253352\n",
      "2023-03-17 22:02:45,983 - INFO - step: 183, loss:0.49332872\n",
      "INFO:train:step: 183, loss:0.49332872\n",
      "2023-03-17 22:02:46,197 - INFO - step: 184, loss:0.48709011\n",
      "INFO:train:step: 184, loss:0.48709011\n",
      "2023-03-17 22:02:46,411 - INFO - step: 185, loss:0.35161707\n",
      "INFO:train:step: 185, loss:0.35161707\n",
      "2023-03-17 22:02:46,625 - INFO - step: 186, loss:0.56860989\n",
      "INFO:train:step: 186, loss:0.56860989\n",
      "2023-03-17 22:02:46,839 - INFO - step: 187, loss:0.58281612\n",
      "INFO:train:step: 187, loss:0.58281612\n",
      "2023-03-17 22:02:47,053 - INFO - step: 188, loss:0.91286075\n",
      "INFO:train:step: 188, loss:0.91286075\n",
      "2023-03-17 22:02:47,267 - INFO - step: 189, loss:0.86780214\n",
      "INFO:train:step: 189, loss:0.86780214\n",
      "2023-03-17 22:02:47,481 - INFO - step: 190, loss:0.54115957\n",
      "INFO:train:step: 190, loss:0.54115957\n",
      "2023-03-17 22:02:47,694 - INFO - step: 191, loss:0.38076827\n",
      "INFO:train:step: 191, loss:0.38076827\n",
      "2023-03-17 22:02:47,907 - INFO - step: 192, loss:0.84031200\n",
      "INFO:train:step: 192, loss:0.84031200\n",
      "2023-03-17 22:02:48,121 - INFO - step: 193, loss:0.38778326\n",
      "INFO:train:step: 193, loss:0.38778326\n",
      "2023-03-17 22:02:48,335 - INFO - step: 194, loss:0.53454530\n",
      "INFO:train:step: 194, loss:0.53454530\n",
      "2023-03-17 22:02:48,549 - INFO - step: 195, loss:0.43108490\n",
      "INFO:train:step: 195, loss:0.43108490\n",
      "2023-03-17 22:02:48,762 - INFO - step: 196, loss:0.49180913\n",
      "INFO:train:step: 196, loss:0.49180913\n",
      "2023-03-17 22:02:48,976 - INFO - step: 197, loss:0.21933499\n",
      "INFO:train:step: 197, loss:0.21933499\n",
      "2023-03-17 22:02:49,190 - INFO - step: 198, loss:0.56858814\n",
      "INFO:train:step: 198, loss:0.56858814\n",
      "2023-03-17 22:02:49,403 - INFO - step: 199, loss:0.57438445\n",
      "INFO:train:step: 199, loss:0.57438445\n",
      "2023-03-17 22:02:49,617 - INFO - step: 200, loss:0.39689845\n",
      "INFO:train:step: 200, loss:0.39689845\n",
      "2023-03-17 22:03:10,582 - INFO - step:200, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:200, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:03:10,662 - INFO - step: 201, loss:0.61704153\n",
      "INFO:train:step: 201, loss:0.61704153\n",
      "2023-03-17 22:03:10,877 - INFO - step: 202, loss:0.82771856\n",
      "INFO:train:step: 202, loss:0.82771856\n",
      "2023-03-17 22:03:11,090 - INFO - step: 203, loss:0.44287995\n",
      "INFO:train:step: 203, loss:0.44287995\n",
      "2023-03-17 22:03:11,304 - INFO - step: 204, loss:0.60580140\n",
      "INFO:train:step: 204, loss:0.60580140\n",
      "2023-03-17 22:03:11,518 - INFO - step: 205, loss:1.02354658\n",
      "INFO:train:step: 205, loss:1.02354658\n",
      "2023-03-17 22:03:11,731 - INFO - step: 206, loss:0.55702925\n",
      "INFO:train:step: 206, loss:0.55702925\n",
      "2023-03-17 22:03:11,945 - INFO - step: 207, loss:0.53235686\n",
      "INFO:train:step: 207, loss:0.53235686\n",
      "2023-03-17 22:03:12,159 - INFO - step: 208, loss:0.41786882\n",
      "INFO:train:step: 208, loss:0.41786882\n",
      "2023-03-17 22:03:12,372 - INFO - step: 209, loss:0.33415765\n",
      "INFO:train:step: 209, loss:0.33415765\n",
      "2023-03-17 22:03:12,587 - INFO - step: 210, loss:0.42007655\n",
      "INFO:train:step: 210, loss:0.42007655\n",
      "2023-03-17 22:03:12,801 - INFO - step: 211, loss:0.35429296\n",
      "INFO:train:step: 211, loss:0.35429296\n",
      "2023-03-17 22:03:13,015 - INFO - step: 212, loss:1.02184439\n",
      "INFO:train:step: 212, loss:1.02184439\n",
      "2023-03-17 22:03:13,229 - INFO - step: 213, loss:0.53878081\n",
      "INFO:train:step: 213, loss:0.53878081\n",
      "2023-03-17 22:03:13,443 - INFO - step: 214, loss:0.56682897\n",
      "INFO:train:step: 214, loss:0.56682897\n",
      "2023-03-17 22:03:13,657 - INFO - step: 215, loss:0.53524894\n",
      "INFO:train:step: 215, loss:0.53524894\n",
      "2023-03-17 22:03:13,871 - INFO - step: 216, loss:0.30297768\n",
      "INFO:train:step: 216, loss:0.30297768\n",
      "2023-03-17 22:03:14,085 - INFO - step: 217, loss:0.45333287\n",
      "INFO:train:step: 217, loss:0.45333287\n",
      "2023-03-17 22:03:14,299 - INFO - step: 218, loss:0.30937198\n",
      "INFO:train:step: 218, loss:0.30937198\n",
      "2023-03-17 22:03:14,513 - INFO - step: 219, loss:0.44925058\n",
      "INFO:train:step: 219, loss:0.44925058\n",
      "2023-03-17 22:03:14,727 - INFO - step: 220, loss:0.73512590\n",
      "INFO:train:step: 220, loss:0.73512590\n",
      "2023-03-17 22:03:14,940 - INFO - step: 221, loss:0.61045861\n",
      "INFO:train:step: 221, loss:0.61045861\n",
      "2023-03-17 22:03:15,154 - INFO - step: 222, loss:0.52693808\n",
      "INFO:train:step: 222, loss:0.52693808\n",
      "2023-03-17 22:03:15,369 - INFO - step: 223, loss:0.97549224\n",
      "INFO:train:step: 223, loss:0.97549224\n",
      "2023-03-17 22:03:15,583 - INFO - step: 224, loss:0.39529192\n",
      "INFO:train:step: 224, loss:0.39529192\n",
      "2023-03-17 22:03:15,797 - INFO - step: 225, loss:0.79905695\n",
      "INFO:train:step: 225, loss:0.79905695\n",
      "2023-03-17 22:03:16,011 - INFO - step: 226, loss:0.73920709\n",
      "INFO:train:step: 226, loss:0.73920709\n",
      "2023-03-17 22:03:16,225 - INFO - step: 227, loss:0.36162275\n",
      "INFO:train:step: 227, loss:0.36162275\n",
      "2023-03-17 22:03:16,439 - INFO - step: 228, loss:0.55850989\n",
      "INFO:train:step: 228, loss:0.55850989\n",
      "2023-03-17 22:03:16,653 - INFO - step: 229, loss:0.45102021\n",
      "INFO:train:step: 229, loss:0.45102021\n",
      "2023-03-17 22:03:16,867 - INFO - step: 230, loss:0.64029008\n",
      "INFO:train:step: 230, loss:0.64029008\n",
      "2023-03-17 22:03:17,081 - INFO - step: 231, loss:0.48964682\n",
      "INFO:train:step: 231, loss:0.48964682\n",
      "2023-03-17 22:03:17,295 - INFO - step: 232, loss:0.52554452\n",
      "INFO:train:step: 232, loss:0.52554452\n",
      "2023-03-17 22:03:17,509 - INFO - step: 233, loss:0.73062241\n",
      "INFO:train:step: 233, loss:0.73062241\n",
      "2023-03-17 22:03:17,723 - INFO - step: 234, loss:0.73276806\n",
      "INFO:train:step: 234, loss:0.73276806\n",
      "2023-03-17 22:03:17,937 - INFO - step: 235, loss:0.65262830\n",
      "INFO:train:step: 235, loss:0.65262830\n",
      "2023-03-17 22:03:18,151 - INFO - step: 236, loss:0.74224049\n",
      "INFO:train:step: 236, loss:0.74224049\n",
      "2023-03-17 22:03:18,365 - INFO - step: 237, loss:0.59652776\n",
      "INFO:train:step: 237, loss:0.59652776\n",
      "2023-03-17 22:03:18,579 - INFO - step: 238, loss:0.64934748\n",
      "INFO:train:step: 238, loss:0.64934748\n",
      "2023-03-17 22:03:18,793 - INFO - step: 239, loss:0.57957655\n",
      "INFO:train:step: 239, loss:0.57957655\n",
      "2023-03-17 22:03:19,007 - INFO - step: 240, loss:0.68671989\n",
      "INFO:train:step: 240, loss:0.68671989\n",
      "2023-03-17 22:03:19,222 - INFO - step: 241, loss:0.53190088\n",
      "INFO:train:step: 241, loss:0.53190088\n",
      "2023-03-17 22:03:19,436 - INFO - step: 242, loss:0.56682342\n",
      "INFO:train:step: 242, loss:0.56682342\n",
      "2023-03-17 22:03:19,650 - INFO - step: 243, loss:0.57791424\n",
      "INFO:train:step: 243, loss:0.57791424\n",
      "2023-03-17 22:03:19,864 - INFO - step: 244, loss:0.56180477\n",
      "INFO:train:step: 244, loss:0.56180477\n",
      "2023-03-17 22:03:20,078 - INFO - step: 245, loss:0.50704658\n",
      "INFO:train:step: 245, loss:0.50704658\n",
      "2023-03-17 22:03:20,292 - INFO - step: 246, loss:0.50971460\n",
      "INFO:train:step: 246, loss:0.50971460\n",
      "2023-03-17 22:03:20,506 - INFO - step: 247, loss:0.57204533\n",
      "INFO:train:step: 247, loss:0.57204533\n",
      "2023-03-17 22:03:20,720 - INFO - step: 248, loss:0.60086030\n",
      "INFO:train:step: 248, loss:0.60086030\n",
      "2023-03-17 22:03:20,934 - INFO - step: 249, loss:0.56673205\n",
      "INFO:train:step: 249, loss:0.56673205\n",
      "2023-03-17 22:03:21,148 - INFO - step: 250, loss:0.56106645\n",
      "INFO:train:step: 250, loss:0.56106645\n",
      "2023-03-17 22:03:42,127 - INFO - step:250, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:250, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:03:42,208 - INFO - step: 251, loss:0.48886371\n",
      "INFO:train:step: 251, loss:0.48886371\n",
      "2023-03-17 22:03:42,422 - INFO - step: 252, loss:0.81517500\n",
      "INFO:train:step: 252, loss:0.81517500\n",
      "2023-03-17 22:03:42,636 - INFO - step: 253, loss:0.63800418\n",
      "INFO:train:step: 253, loss:0.63800418\n",
      "2023-03-17 22:03:42,850 - INFO - step: 254, loss:0.71440625\n",
      "INFO:train:step: 254, loss:0.71440625\n",
      "2023-03-17 22:03:43,064 - INFO - step: 255, loss:0.64898962\n",
      "INFO:train:step: 255, loss:0.64898962\n",
      "2023-03-17 22:03:43,278 - INFO - step: 256, loss:0.63759124\n",
      "INFO:train:step: 256, loss:0.63759124\n",
      "2023-03-17 22:03:43,492 - INFO - step: 257, loss:0.59248382\n",
      "INFO:train:step: 257, loss:0.59248382\n",
      "2023-03-17 22:03:43,706 - INFO - step: 258, loss:0.59963584\n",
      "INFO:train:step: 258, loss:0.59963584\n",
      "2023-03-17 22:03:43,920 - INFO - step: 259, loss:0.73393345\n",
      "INFO:train:step: 259, loss:0.73393345\n",
      "2023-03-17 22:03:44,134 - INFO - step: 260, loss:0.65942419\n",
      "INFO:train:step: 260, loss:0.65942419\n",
      "2023-03-17 22:03:44,348 - INFO - step: 261, loss:0.61790067\n",
      "INFO:train:step: 261, loss:0.61790067\n",
      "2023-03-17 22:03:44,562 - INFO - step: 262, loss:0.51243019\n",
      "INFO:train:step: 262, loss:0.51243019\n",
      "2023-03-17 22:03:44,776 - INFO - step: 263, loss:0.64696026\n",
      "INFO:train:step: 263, loss:0.64696026\n",
      "2023-03-17 22:03:44,990 - INFO - step: 264, loss:0.69013780\n",
      "INFO:train:step: 264, loss:0.69013780\n",
      "2023-03-17 22:03:45,204 - INFO - step: 265, loss:0.62634736\n",
      "INFO:train:step: 265, loss:0.62634736\n",
      "2023-03-17 22:03:45,418 - INFO - step: 266, loss:0.58105975\n",
      "INFO:train:step: 266, loss:0.58105975\n",
      "2023-03-17 22:03:45,632 - INFO - step: 267, loss:0.58424419\n",
      "INFO:train:step: 267, loss:0.58424419\n",
      "2023-03-17 22:03:45,806 - INFO - step: 268, loss:0.70422226\n",
      "INFO:train:step: 268, loss:0.70422226\n",
      "2023-03-17 22:03:45,944 - INFO - step: 269, loss:0.53892809\n",
      "INFO:train:step: 269, loss:0.53892809\n",
      "2023-03-17 22:03:46,158 - INFO - step: 270, loss:0.62217569\n",
      "INFO:train:step: 270, loss:0.62217569\n",
      "2023-03-17 22:03:46,372 - INFO - step: 271, loss:0.55757535\n",
      "INFO:train:step: 271, loss:0.55757535\n",
      "2023-03-17 22:03:46,586 - INFO - step: 272, loss:0.60832191\n",
      "INFO:train:step: 272, loss:0.60832191\n",
      "2023-03-17 22:03:46,800 - INFO - step: 273, loss:0.79927564\n",
      "INFO:train:step: 273, loss:0.79927564\n",
      "2023-03-17 22:03:47,013 - INFO - step: 274, loss:0.69020283\n",
      "INFO:train:step: 274, loss:0.69020283\n",
      "2023-03-17 22:03:47,227 - INFO - step: 275, loss:0.70670378\n",
      "INFO:train:step: 275, loss:0.70670378\n",
      "2023-03-17 22:03:47,441 - INFO - step: 276, loss:0.57649946\n",
      "INFO:train:step: 276, loss:0.57649946\n",
      "2023-03-17 22:03:47,654 - INFO - step: 277, loss:0.70312387\n",
      "INFO:train:step: 277, loss:0.70312387\n",
      "2023-03-17 22:03:47,867 - INFO - step: 278, loss:0.72166866\n",
      "INFO:train:step: 278, loss:0.72166866\n",
      "2023-03-17 22:03:48,081 - INFO - step: 279, loss:0.58772576\n",
      "INFO:train:step: 279, loss:0.58772576\n",
      "2023-03-17 22:03:48,294 - INFO - step: 280, loss:0.62418038\n",
      "INFO:train:step: 280, loss:0.62418038\n",
      "2023-03-17 22:03:48,508 - INFO - step: 281, loss:0.66160351\n",
      "INFO:train:step: 281, loss:0.66160351\n",
      "2023-03-17 22:03:48,721 - INFO - step: 282, loss:0.68110889\n",
      "INFO:train:step: 282, loss:0.68110889\n",
      "2023-03-17 22:03:48,935 - INFO - step: 283, loss:0.60073125\n",
      "INFO:train:step: 283, loss:0.60073125\n",
      "2023-03-17 22:03:49,149 - INFO - step: 284, loss:0.61315876\n",
      "INFO:train:step: 284, loss:0.61315876\n",
      "2023-03-17 22:03:49,362 - INFO - step: 285, loss:0.64904302\n",
      "INFO:train:step: 285, loss:0.64904302\n",
      "2023-03-17 22:03:49,576 - INFO - step: 286, loss:0.45683399\n",
      "INFO:train:step: 286, loss:0.45683399\n",
      "2023-03-17 22:03:49,790 - INFO - step: 287, loss:0.72911173\n",
      "INFO:train:step: 287, loss:0.72911173\n",
      "2023-03-17 22:03:50,003 - INFO - step: 288, loss:0.53541327\n",
      "INFO:train:step: 288, loss:0.53541327\n",
      "2023-03-17 22:03:50,217 - INFO - step: 289, loss:0.53335345\n",
      "INFO:train:step: 289, loss:0.53335345\n",
      "2023-03-17 22:03:50,431 - INFO - step: 290, loss:0.62236726\n",
      "INFO:train:step: 290, loss:0.62236726\n",
      "2023-03-17 22:03:50,644 - INFO - step: 291, loss:0.83674717\n",
      "INFO:train:step: 291, loss:0.83674717\n",
      "2023-03-17 22:03:50,858 - INFO - step: 292, loss:0.81280518\n",
      "INFO:train:step: 292, loss:0.81280518\n",
      "2023-03-17 22:03:51,071 - INFO - step: 293, loss:0.67985082\n",
      "INFO:train:step: 293, loss:0.67985082\n",
      "2023-03-17 22:03:51,285 - INFO - step: 294, loss:0.73179018\n",
      "INFO:train:step: 294, loss:0.73179018\n",
      "2023-03-17 22:03:51,498 - INFO - step: 295, loss:0.74148405\n",
      "INFO:train:step: 295, loss:0.74148405\n",
      "2023-03-17 22:03:51,711 - INFO - step: 296, loss:0.80355757\n",
      "INFO:train:step: 296, loss:0.80355757\n",
      "2023-03-17 22:03:51,925 - INFO - step: 297, loss:0.70911247\n",
      "INFO:train:step: 297, loss:0.70911247\n",
      "2023-03-17 22:03:52,138 - INFO - step: 298, loss:0.59227949\n",
      "INFO:train:step: 298, loss:0.59227949\n",
      "2023-03-17 22:03:52,352 - INFO - step: 299, loss:0.69383061\n",
      "INFO:train:step: 299, loss:0.69383061\n",
      "2023-03-17 22:03:52,566 - INFO - step: 300, loss:0.57627785\n",
      "INFO:train:step: 300, loss:0.57627785\n",
      "2023-03-17 22:04:13,490 - INFO - step:300, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:300, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:04:13,570 - INFO - step: 301, loss:0.56073558\n",
      "INFO:train:step: 301, loss:0.56073558\n",
      "2023-03-17 22:04:13,784 - INFO - step: 302, loss:0.61939842\n",
      "INFO:train:step: 302, loss:0.61939842\n",
      "2023-03-17 22:04:13,998 - INFO - step: 303, loss:0.64369774\n",
      "INFO:train:step: 303, loss:0.64369774\n",
      "2023-03-17 22:04:14,212 - INFO - step: 304, loss:0.69147140\n",
      "INFO:train:step: 304, loss:0.69147140\n",
      "2023-03-17 22:04:14,426 - INFO - step: 305, loss:0.67888105\n",
      "INFO:train:step: 305, loss:0.67888105\n",
      "2023-03-17 22:04:14,639 - INFO - step: 306, loss:0.63414079\n",
      "INFO:train:step: 306, loss:0.63414079\n",
      "2023-03-17 22:04:14,853 - INFO - step: 307, loss:0.58668786\n",
      "INFO:train:step: 307, loss:0.58668786\n",
      "2023-03-17 22:04:15,068 - INFO - step: 308, loss:0.70012349\n",
      "INFO:train:step: 308, loss:0.70012349\n",
      "2023-03-17 22:04:15,282 - INFO - step: 309, loss:0.67535514\n",
      "INFO:train:step: 309, loss:0.67535514\n",
      "2023-03-17 22:04:15,497 - INFO - step: 310, loss:0.64048594\n",
      "INFO:train:step: 310, loss:0.64048594\n",
      "2023-03-17 22:04:15,710 - INFO - step: 311, loss:0.72709829\n",
      "INFO:train:step: 311, loss:0.72709829\n",
      "2023-03-17 22:04:15,925 - INFO - step: 312, loss:0.68582094\n",
      "INFO:train:step: 312, loss:0.68582094\n",
      "2023-03-17 22:04:16,138 - INFO - step: 313, loss:0.63401550\n",
      "INFO:train:step: 313, loss:0.63401550\n",
      "2023-03-17 22:04:16,353 - INFO - step: 314, loss:0.74231225\n",
      "INFO:train:step: 314, loss:0.74231225\n",
      "2023-03-17 22:04:16,567 - INFO - step: 315, loss:0.55368578\n",
      "INFO:train:step: 315, loss:0.55368578\n",
      "2023-03-17 22:04:16,782 - INFO - step: 316, loss:0.60945565\n",
      "INFO:train:step: 316, loss:0.60945565\n",
      "2023-03-17 22:04:16,995 - INFO - step: 317, loss:0.60744357\n",
      "INFO:train:step: 317, loss:0.60744357\n",
      "2023-03-17 22:04:17,210 - INFO - step: 318, loss:0.59185648\n",
      "INFO:train:step: 318, loss:0.59185648\n",
      "2023-03-17 22:04:17,426 - INFO - step: 319, loss:0.56360960\n",
      "INFO:train:step: 319, loss:0.56360960\n",
      "2023-03-17 22:04:17,643 - INFO - step: 320, loss:0.81388831\n",
      "INFO:train:step: 320, loss:0.81388831\n",
      "2023-03-17 22:04:17,859 - INFO - step: 321, loss:0.73053885\n",
      "INFO:train:step: 321, loss:0.73053885\n",
      "2023-03-17 22:04:18,075 - INFO - step: 322, loss:0.79296541\n",
      "INFO:train:step: 322, loss:0.79296541\n",
      "2023-03-17 22:04:18,290 - INFO - step: 323, loss:0.66106421\n",
      "INFO:train:step: 323, loss:0.66106421\n",
      "2023-03-17 22:04:18,505 - INFO - step: 324, loss:0.65841031\n",
      "INFO:train:step: 324, loss:0.65841031\n",
      "2023-03-17 22:04:18,719 - INFO - step: 325, loss:0.69539100\n",
      "INFO:train:step: 325, loss:0.69539100\n",
      "2023-03-17 22:04:18,934 - INFO - step: 326, loss:0.72168386\n",
      "INFO:train:step: 326, loss:0.72168386\n",
      "2023-03-17 22:04:19,149 - INFO - step: 327, loss:0.68250287\n",
      "INFO:train:step: 327, loss:0.68250287\n",
      "2023-03-17 22:04:19,363 - INFO - step: 328, loss:0.68867034\n",
      "INFO:train:step: 328, loss:0.68867034\n",
      "2023-03-17 22:04:19,577 - INFO - step: 329, loss:0.66905564\n",
      "INFO:train:step: 329, loss:0.66905564\n",
      "2023-03-17 22:04:19,794 - INFO - step: 330, loss:0.68351012\n",
      "INFO:train:step: 330, loss:0.68351012\n",
      "2023-03-17 22:04:20,007 - INFO - step: 331, loss:0.59774148\n",
      "INFO:train:step: 331, loss:0.59774148\n",
      "2023-03-17 22:04:20,222 - INFO - step: 332, loss:0.70731676\n",
      "INFO:train:step: 332, loss:0.70731676\n",
      "2023-03-17 22:04:20,436 - INFO - step: 333, loss:0.64452428\n",
      "INFO:train:step: 333, loss:0.64452428\n",
      "2023-03-17 22:04:20,651 - INFO - step: 334, loss:0.65299624\n",
      "INFO:train:step: 334, loss:0.65299624\n",
      "2023-03-17 22:04:20,865 - INFO - step: 335, loss:0.60299534\n",
      "INFO:train:step: 335, loss:0.60299534\n",
      "2023-03-17 22:04:21,079 - INFO - step: 336, loss:0.63896775\n",
      "INFO:train:step: 336, loss:0.63896775\n",
      "2023-03-17 22:04:21,294 - INFO - step: 337, loss:0.64065635\n",
      "INFO:train:step: 337, loss:0.64065635\n",
      "2023-03-17 22:04:21,509 - INFO - step: 338, loss:0.54074013\n",
      "INFO:train:step: 338, loss:0.54074013\n",
      "2023-03-17 22:04:21,725 - INFO - step: 339, loss:0.54566550\n",
      "INFO:train:step: 339, loss:0.54566550\n",
      "2023-03-17 22:04:21,941 - INFO - step: 340, loss:0.64284170\n",
      "INFO:train:step: 340, loss:0.64284170\n",
      "2023-03-17 22:04:22,155 - INFO - step: 341, loss:0.64548469\n",
      "INFO:train:step: 341, loss:0.64548469\n",
      "2023-03-17 22:04:22,370 - INFO - step: 342, loss:0.65277743\n",
      "INFO:train:step: 342, loss:0.65277743\n",
      "2023-03-17 22:04:22,584 - INFO - step: 343, loss:0.50002623\n",
      "INFO:train:step: 343, loss:0.50002623\n",
      "2023-03-17 22:04:22,799 - INFO - step: 344, loss:0.43998063\n",
      "INFO:train:step: 344, loss:0.43998063\n",
      "2023-03-17 22:04:23,013 - INFO - step: 345, loss:0.45809051\n",
      "INFO:train:step: 345, loss:0.45809051\n",
      "2023-03-17 22:04:23,228 - INFO - step: 346, loss:0.52885497\n",
      "INFO:train:step: 346, loss:0.52885497\n",
      "2023-03-17 22:04:23,442 - INFO - step: 347, loss:0.69106174\n",
      "INFO:train:step: 347, loss:0.69106174\n",
      "2023-03-17 22:04:23,656 - INFO - step: 348, loss:1.04257560\n",
      "INFO:train:step: 348, loss:1.04257560\n",
      "2023-03-17 22:04:23,871 - INFO - step: 349, loss:0.80998063\n",
      "INFO:train:step: 349, loss:0.80998063\n",
      "2023-03-17 22:04:24,085 - INFO - step: 350, loss:0.88396913\n",
      "INFO:train:step: 350, loss:0.88396913\n",
      "2023-03-17 22:04:45,315 - INFO - step:350, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:350, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:04:45,393 - INFO - step: 351, loss:0.56016958\n",
      "INFO:train:step: 351, loss:0.56016958\n",
      "2023-03-17 22:04:45,606 - INFO - step: 352, loss:0.75707036\n",
      "INFO:train:step: 352, loss:0.75707036\n",
      "2023-03-17 22:04:45,819 - INFO - step: 353, loss:0.62665099\n",
      "INFO:train:step: 353, loss:0.62665099\n",
      "2023-03-17 22:04:46,031 - INFO - step: 354, loss:0.87142336\n",
      "INFO:train:step: 354, loss:0.87142336\n",
      "2023-03-17 22:04:46,244 - INFO - step: 355, loss:0.64218825\n",
      "INFO:train:step: 355, loss:0.64218825\n",
      "2023-03-17 22:04:46,456 - INFO - step: 356, loss:0.75445431\n",
      "INFO:train:step: 356, loss:0.75445431\n",
      "2023-03-17 22:04:46,669 - INFO - step: 357, loss:0.70733142\n",
      "INFO:train:step: 357, loss:0.70733142\n",
      "2023-03-17 22:04:46,881 - INFO - step: 358, loss:0.57645959\n",
      "INFO:train:step: 358, loss:0.57645959\n",
      "2023-03-17 22:04:47,093 - INFO - step: 359, loss:0.62206548\n",
      "INFO:train:step: 359, loss:0.62206548\n",
      "2023-03-17 22:04:47,306 - INFO - step: 360, loss:0.59048712\n",
      "INFO:train:step: 360, loss:0.59048712\n",
      "2023-03-17 22:04:47,518 - INFO - step: 361, loss:0.70739996\n",
      "INFO:train:step: 361, loss:0.70739996\n",
      "2023-03-17 22:04:47,731 - INFO - step: 362, loss:0.62269175\n",
      "INFO:train:step: 362, loss:0.62269175\n",
      "2023-03-17 22:04:47,943 - INFO - step: 363, loss:0.64340985\n",
      "INFO:train:step: 363, loss:0.64340985\n",
      "2023-03-17 22:04:48,155 - INFO - step: 364, loss:0.56485957\n",
      "INFO:train:step: 364, loss:0.56485957\n",
      "2023-03-17 22:04:48,368 - INFO - step: 365, loss:0.61543030\n",
      "INFO:train:step: 365, loss:0.61543030\n",
      "2023-03-17 22:04:48,580 - INFO - step: 366, loss:0.75437140\n",
      "INFO:train:step: 366, loss:0.75437140\n",
      "2023-03-17 22:04:48,793 - INFO - step: 367, loss:0.61297601\n",
      "INFO:train:step: 367, loss:0.61297601\n",
      "2023-03-17 22:04:49,006 - INFO - step: 368, loss:0.68313652\n",
      "INFO:train:step: 368, loss:0.68313652\n",
      "2023-03-17 22:04:49,218 - INFO - step: 369, loss:0.59775448\n",
      "INFO:train:step: 369, loss:0.59775448\n",
      "2023-03-17 22:04:49,433 - INFO - step: 370, loss:0.57939553\n",
      "INFO:train:step: 370, loss:0.57939553\n",
      "2023-03-17 22:04:49,646 - INFO - step: 371, loss:0.71050894\n",
      "INFO:train:step: 371, loss:0.71050894\n",
      "2023-03-17 22:04:49,859 - INFO - step: 372, loss:0.50414056\n",
      "INFO:train:step: 372, loss:0.50414056\n",
      "2023-03-17 22:04:50,071 - INFO - step: 373, loss:0.50555557\n",
      "INFO:train:step: 373, loss:0.50555557\n",
      "2023-03-17 22:04:50,284 - INFO - step: 374, loss:0.49003461\n",
      "INFO:train:step: 374, loss:0.49003461\n",
      "2023-03-17 22:04:50,497 - INFO - step: 375, loss:0.65201896\n",
      "INFO:train:step: 375, loss:0.65201896\n",
      "2023-03-17 22:04:50,709 - INFO - step: 376, loss:0.59012264\n",
      "INFO:train:step: 376, loss:0.59012264\n",
      "2023-03-17 22:04:50,922 - INFO - step: 377, loss:0.82388115\n",
      "INFO:train:step: 377, loss:0.82388115\n",
      "2023-03-17 22:04:51,135 - INFO - step: 378, loss:0.53364962\n",
      "INFO:train:step: 378, loss:0.53364962\n",
      "2023-03-17 22:04:51,348 - INFO - step: 379, loss:0.67266566\n",
      "INFO:train:step: 379, loss:0.67266566\n",
      "2023-03-17 22:04:51,562 - INFO - step: 380, loss:0.62718028\n",
      "INFO:train:step: 380, loss:0.62718028\n",
      "2023-03-17 22:04:51,775 - INFO - step: 381, loss:0.50298607\n",
      "INFO:train:step: 381, loss:0.50298607\n",
      "2023-03-17 22:04:51,988 - INFO - step: 382, loss:0.56527519\n",
      "INFO:train:step: 382, loss:0.56527519\n",
      "2023-03-17 22:04:52,200 - INFO - step: 383, loss:0.65996063\n",
      "INFO:train:step: 383, loss:0.65996063\n",
      "2023-03-17 22:04:52,413 - INFO - step: 384, loss:0.43273565\n",
      "INFO:train:step: 384, loss:0.43273565\n",
      "2023-03-17 22:04:52,626 - INFO - step: 385, loss:0.49798238\n",
      "INFO:train:step: 385, loss:0.49798238\n",
      "2023-03-17 22:04:52,839 - INFO - step: 386, loss:0.46178907\n",
      "INFO:train:step: 386, loss:0.46178907\n",
      "2023-03-17 22:04:53,052 - INFO - step: 387, loss:0.35634422\n",
      "INFO:train:step: 387, loss:0.35634422\n",
      "2023-03-17 22:04:53,265 - INFO - step: 388, loss:0.41449893\n",
      "INFO:train:step: 388, loss:0.41449893\n",
      "2023-03-17 22:04:53,478 - INFO - step: 389, loss:0.44829801\n",
      "INFO:train:step: 389, loss:0.44829801\n",
      "2023-03-17 22:04:53,691 - INFO - step: 390, loss:0.80439055\n",
      "INFO:train:step: 390, loss:0.80439055\n",
      "2023-03-17 22:04:53,904 - INFO - step: 391, loss:0.79432064\n",
      "INFO:train:step: 391, loss:0.79432064\n",
      "2023-03-17 22:04:54,117 - INFO - step: 392, loss:0.30328488\n",
      "INFO:train:step: 392, loss:0.30328488\n",
      "2023-03-17 22:04:54,330 - INFO - step: 393, loss:0.74415457\n",
      "INFO:train:step: 393, loss:0.74415457\n",
      "2023-03-17 22:04:54,543 - INFO - step: 394, loss:0.85311925\n",
      "INFO:train:step: 394, loss:0.85311925\n",
      "2023-03-17 22:04:54,755 - INFO - step: 395, loss:0.52597600\n",
      "INFO:train:step: 395, loss:0.52597600\n",
      "2023-03-17 22:04:54,968 - INFO - step: 396, loss:0.48294955\n",
      "INFO:train:step: 396, loss:0.48294955\n",
      "2023-03-17 22:04:55,181 - INFO - step: 397, loss:0.86752146\n",
      "INFO:train:step: 397, loss:0.86752146\n",
      "2023-03-17 22:04:55,394 - INFO - step: 398, loss:0.62530303\n",
      "INFO:train:step: 398, loss:0.62530303\n",
      "2023-03-17 22:04:55,606 - INFO - step: 399, loss:0.53652680\n",
      "INFO:train:step: 399, loss:0.53652680\n",
      "2023-03-17 22:04:55,819 - INFO - step: 400, loss:0.63243216\n",
      "INFO:train:step: 400, loss:0.63243216\n",
      "2023-03-17 22:05:16,931 - INFO - step:400, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:400, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:05:17,011 - INFO - step: 401, loss:0.59013790\n",
      "INFO:train:step: 401, loss:0.59013790\n",
      "2023-03-17 22:05:17,225 - INFO - step: 402, loss:0.69726622\n",
      "INFO:train:step: 402, loss:0.69726622\n",
      "2023-03-17 22:05:17,440 - INFO - step: 403, loss:0.61369783\n",
      "INFO:train:step: 403, loss:0.61369783\n",
      "2023-03-17 22:05:17,654 - INFO - step: 404, loss:0.80435038\n",
      "INFO:train:step: 404, loss:0.80435038\n",
      "2023-03-17 22:05:17,868 - INFO - step: 405, loss:0.65799534\n",
      "INFO:train:step: 405, loss:0.65799534\n",
      "2023-03-17 22:05:18,082 - INFO - step: 406, loss:0.73678696\n",
      "INFO:train:step: 406, loss:0.73678696\n",
      "2023-03-17 22:05:18,296 - INFO - step: 407, loss:0.59380609\n",
      "INFO:train:step: 407, loss:0.59380609\n",
      "2023-03-17 22:05:18,509 - INFO - step: 408, loss:0.74363887\n",
      "INFO:train:step: 408, loss:0.74363887\n",
      "2023-03-17 22:05:18,723 - INFO - step: 409, loss:0.65130830\n",
      "INFO:train:step: 409, loss:0.65130830\n",
      "2023-03-17 22:05:18,937 - INFO - step: 410, loss:0.59639859\n",
      "INFO:train:step: 410, loss:0.59639859\n",
      "2023-03-17 22:05:19,151 - INFO - step: 411, loss:0.73527354\n",
      "INFO:train:step: 411, loss:0.73527354\n",
      "2023-03-17 22:05:19,366 - INFO - step: 412, loss:0.60321736\n",
      "INFO:train:step: 412, loss:0.60321736\n",
      "2023-03-17 22:05:19,580 - INFO - step: 413, loss:0.64537072\n",
      "INFO:train:step: 413, loss:0.64537072\n",
      "2023-03-17 22:05:19,797 - INFO - step: 414, loss:0.60627985\n",
      "INFO:train:step: 414, loss:0.60627985\n",
      "2023-03-17 22:05:20,011 - INFO - step: 415, loss:0.61215025\n",
      "INFO:train:step: 415, loss:0.61215025\n",
      "2023-03-17 22:05:20,225 - INFO - step: 416, loss:0.60866791\n",
      "INFO:train:step: 416, loss:0.60866791\n",
      "2023-03-17 22:05:20,439 - INFO - step: 417, loss:0.61526477\n",
      "INFO:train:step: 417, loss:0.61526477\n",
      "2023-03-17 22:05:20,652 - INFO - step: 418, loss:0.56087470\n",
      "INFO:train:step: 418, loss:0.56087470\n",
      "2023-03-17 22:05:20,866 - INFO - step: 419, loss:0.64325708\n",
      "INFO:train:step: 419, loss:0.64325708\n",
      "2023-03-17 22:05:21,080 - INFO - step: 420, loss:0.62364340\n",
      "INFO:train:step: 420, loss:0.62364340\n",
      "2023-03-17 22:05:21,294 - INFO - step: 421, loss:0.62488174\n",
      "INFO:train:step: 421, loss:0.62488174\n",
      "2023-03-17 22:05:21,508 - INFO - step: 422, loss:0.59822720\n",
      "INFO:train:step: 422, loss:0.59822720\n",
      "2023-03-17 22:05:21,722 - INFO - step: 423, loss:0.59535152\n",
      "INFO:train:step: 423, loss:0.59535152\n",
      "2023-03-17 22:05:21,936 - INFO - step: 424, loss:0.64812708\n",
      "INFO:train:step: 424, loss:0.64812708\n",
      "2023-03-17 22:05:22,150 - INFO - step: 425, loss:0.56835634\n",
      "INFO:train:step: 425, loss:0.56835634\n",
      "2023-03-17 22:05:22,364 - INFO - step: 426, loss:0.44322616\n",
      "INFO:train:step: 426, loss:0.44322616\n",
      "2023-03-17 22:05:22,578 - INFO - step: 427, loss:0.46875316\n",
      "INFO:train:step: 427, loss:0.46875316\n",
      "2023-03-17 22:05:22,791 - INFO - step: 428, loss:0.66422361\n",
      "INFO:train:step: 428, loss:0.66422361\n",
      "2023-03-17 22:05:23,006 - INFO - step: 429, loss:0.62935436\n",
      "INFO:train:step: 429, loss:0.62935436\n",
      "2023-03-17 22:05:23,220 - INFO - step: 430, loss:0.45243183\n",
      "INFO:train:step: 430, loss:0.45243183\n",
      "2023-03-17 22:05:23,434 - INFO - step: 431, loss:0.38313392\n",
      "INFO:train:step: 431, loss:0.38313392\n",
      "2023-03-17 22:05:23,648 - INFO - step: 432, loss:0.56837511\n",
      "INFO:train:step: 432, loss:0.56837511\n",
      "2023-03-17 22:05:23,862 - INFO - step: 433, loss:0.56105787\n",
      "INFO:train:step: 433, loss:0.56105787\n",
      "2023-03-17 22:05:24,076 - INFO - step: 434, loss:0.78232378\n",
      "INFO:train:step: 434, loss:0.78232378\n",
      "2023-03-17 22:05:24,290 - INFO - step: 435, loss:0.56217223\n",
      "INFO:train:step: 435, loss:0.56217223\n",
      "2023-03-17 22:05:24,505 - INFO - step: 436, loss:0.57373130\n",
      "INFO:train:step: 436, loss:0.57373130\n",
      "2023-03-17 22:05:24,719 - INFO - step: 437, loss:0.73288435\n",
      "INFO:train:step: 437, loss:0.73288435\n",
      "2023-03-17 22:05:24,933 - INFO - step: 438, loss:0.80144727\n",
      "INFO:train:step: 438, loss:0.80144727\n",
      "2023-03-17 22:05:25,147 - INFO - step: 439, loss:0.75629544\n",
      "INFO:train:step: 439, loss:0.75629544\n",
      "2023-03-17 22:05:25,361 - INFO - step: 440, loss:0.46294928\n",
      "INFO:train:step: 440, loss:0.46294928\n",
      "2023-03-17 22:05:25,576 - INFO - step: 441, loss:0.69982743\n",
      "INFO:train:step: 441, loss:0.69982743\n",
      "2023-03-17 22:05:25,790 - INFO - step: 442, loss:0.69743252\n",
      "INFO:train:step: 442, loss:0.69743252\n",
      "2023-03-17 22:05:26,003 - INFO - step: 443, loss:0.56579858\n",
      "INFO:train:step: 443, loss:0.56579858\n",
      "2023-03-17 22:05:26,217 - INFO - step: 444, loss:0.56427771\n",
      "INFO:train:step: 444, loss:0.56427771\n",
      "2023-03-17 22:05:26,430 - INFO - step: 445, loss:0.54685599\n",
      "INFO:train:step: 445, loss:0.54685599\n",
      "2023-03-17 22:05:26,643 - INFO - step: 446, loss:0.41619655\n",
      "INFO:train:step: 446, loss:0.41619655\n",
      "2023-03-17 22:05:26,856 - INFO - step: 447, loss:0.44073206\n",
      "INFO:train:step: 447, loss:0.44073206\n",
      "2023-03-17 22:05:27,070 - INFO - step: 448, loss:0.38668367\n",
      "INFO:train:step: 448, loss:0.38668367\n",
      "2023-03-17 22:05:27,283 - INFO - step: 449, loss:0.46039665\n",
      "INFO:train:step: 449, loss:0.46039665\n",
      "2023-03-17 22:05:27,496 - INFO - step: 450, loss:0.47371423\n",
      "INFO:train:step: 450, loss:0.47371423\n",
      "2023-03-17 22:05:48,358 - INFO - step:450, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "INFO:train:step:450, matthews_corr:0.000000, Acc:69.127517%, Train: matthews_corr:0.000000, Acc:70.436206%,\n",
      "2023-03-17 22:05:48,438 - INFO - step: 451, loss:0.50329995\n",
      "INFO:train:step: 451, loss:0.50329995\n",
      "2023-03-17 22:05:48,651 - INFO - step: 452, loss:0.49274525\n",
      "INFO:train:step: 452, loss:0.49274525\n",
      "2023-03-17 22:05:48,864 - INFO - step: 453, loss:0.38589743\n",
      "INFO:train:step: 453, loss:0.38589743\n",
      "2023-03-17 22:05:49,077 - INFO - step: 454, loss:0.56873113\n",
      "INFO:train:step: 454, loss:0.56873113\n",
      "2023-03-17 22:05:49,290 - INFO - step: 455, loss:0.56139570\n",
      "INFO:train:step: 455, loss:0.56139570\n",
      "2023-03-17 22:05:49,503 - INFO - step: 456, loss:0.81209868\n",
      "INFO:train:step: 456, loss:0.81209868\n"
     ]
    }
   ],
   "source": [
    "for i, this_batch_size in enumerate(batch_size_list):\n",
    "    for j, this_scheduler in enumerate(scheduler_list):\n",
    "        for k, this_optimizer in enumerate(optimizer_list):\n",
    "            for m, this_lr in enumerate(lr_list):\n",
    "                loss_list = []\n",
    "                metric_list = []\n",
    "                acc_list = []\n",
    "                train_metric_list = []\n",
    "                train_acc_list = []\n",
    "                model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "                model.to(device)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=this_batch_size)\n",
    "                sche, opt = prepare(this_scheduler, this_optimizer)\n",
    "                optimizer = opt(model.parameters(), lr=this_lr if this_scheduler == 'no' else this_lr * 2)\n",
    "                scheduler = sche(optimizer, num_warmup_steps=int(steps / 10), num_training_steps=steps)\n",
    "                step = 0\n",
    "\n",
    "                metric, acc = evaluate(model, test_dataset)\n",
    "                metric_list.append(metric)\n",
    "                acc_list.append(acc)\n",
    "                tmetric, tacc = evaluate(model, train_dataset)\n",
    "                train_metric_list.append(tmetric)\n",
    "                train_acc_list.append(tacc)\n",
    "                logger.info(f\"step:{step}, matthews_corr:{metric:.6f}, Acc:{acc*100:4f}%, Train: matthews_corr:{tmetric:.6f}, Acc:{tacc*100:4f}%,\")\n",
    "\n",
    "                while True:\n",
    "                    for X in train_loader:\n",
    "                        model.train()\n",
    "                        optimizer.zero_grad()\n",
    "                        batch = {k: v.to(device) for k, v in X.items()}\n",
    "                        loss = model(**batch).loss\n",
    "                        logger.info(f\"step: {step+1}, loss:{loss.item():.8f}\")\n",
    "\n",
    "                        loss_list.append(loss.item())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        step += 1\n",
    "\n",
    "                        if step % report_step == 0:\n",
    "                            metric, acc = evaluate(model, test_dataset)\n",
    "                            metric_list.append(metric)\n",
    "                            acc_list.append(acc)\n",
    "                            tmetric, tacc = evaluate(model, train_dataset)\n",
    "                            train_metric_list.append(tmetric)\n",
    "                            train_acc_list.append(tacc)\n",
    "                            logger.info(f\"step:{step}, matthews_corr:{metric:.6f}, Acc:{acc*100:4f}%, Train: matthews_corr:{tmetric:.6f}, Acc:{tacc*100:4f}%,\")\n",
    "\n",
    "                        if step == steps:\n",
    "                            break\n",
    "                    if step == steps:\n",
    "                        break\n",
    "\n",
    "                file_name = dataset_name + \",batchsize\" + str(this_batch_size) + \",scheduler\" + this_scheduler + \",optimizer\" + str(this_optimizer) + \",LR\" + str(this_lr)\n",
    "                np.save(current_path / (file_name + 'loss.npy'), np.array(loss_list))\n",
    "                np.save(current_path / (file_name + 'metric.npy'), np.array(metric_list))\n",
    "                np.save(current_path / (file_name + 'acc.npy'), np.array(acc_list))\n",
    "                np.save(current_path / (file_name + 'trainmetric.npy'), np.array(train_metric_list))\n",
    "                np.save(current_path / (file_name + 'trainacc.npy'), np.array(train_acc_list))\n",
    "\n",
    "                del model\n",
    "                del optimizer\n",
    "                del scheduler\n",
    "                del train_loader\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
