{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup,get_polynomial_decay_schedule_with_warmup, Adafactor\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from lion_pytorch import Lion\n",
    "from torch.utils.data import DataLoader\n",
    "import GLUE\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "current_path = Path.cwd()\n",
    "dataset_name = 'cola'\n",
    "# lr_list = [3e-5,3e-4,3e-3]\n",
    "# scheduler_list = ['no', 'linear' ,'ord10']\n",
    "# optimizer_list = ['Lion', 'AdamW','AdaFactor']\n",
    "# batch_size_list = [32,64,128]\n",
    "# steps = 50*1000\n",
    "lr_list = [1e-5]\n",
    "scheduler_list = ['no']\n",
    "optimizer_list = ['Lion']\n",
    "batch_size_list = [32]\n",
    "steps = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/Xiang/.cache/huggingface/datasets/mariosasko___glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c1d5907889d4662a756811965946788"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Xiang\\.cache\\huggingface\\datasets\\mariosasko___glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-2bc0360bf4726f4f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Xiang\\.cache\\huggingface\\datasets\\mariosasko___glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-77227c16e4defc4f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Xiang\\.cache\\huggingface\\datasets\\mariosasko___glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\\cache-c61c89821cd722e6.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset,_,test_dataset = GLUE.get_torch_dataset(tokenizer, \"cola\", padding=\"max_length\", truncation=True, max_length = 64)\n",
    "\n",
    "def constant_scheduler(\n",
    "    optimizer, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1\n",
    "):\n",
    "    def lambda_func(step:int):\n",
    "        return 1.\n",
    "\n",
    "    return LambdaLR(optimizer, lambda_func, last_epoch)\n",
    "\n",
    "def prepare(sche, opt):\n",
    "    if sche == 'no':\n",
    "        sches = partial(constant_scheduler)\n",
    "    if sche == 'linear':\n",
    "        sches = partial(get_linear_schedule_with_warmup)\n",
    "    if sche == 'ord10':\n",
    "        sches = partial(get_polynomial_decay_schedule_with_warmup,power = 10.0)\n",
    "\n",
    "    if opt == 'Lion':\n",
    "        opts = partial(Lion, betas = (0.95,0.98), weight_decay = 0.01)\n",
    "    if opt == 'AdaFactor':\n",
    "        opts = partial(Adafactor, weight_decay = 0.001)\n",
    "    if opt == 'AdamW':\n",
    "        opts = partial(torch.optim.AdamW, betas = (0.9,0.99), weight_decay = 0.001)\n",
    "\n",
    "\n",
    "    return sches, opts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, shuffle = False, batch_size = 32)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for: sche:no,opt:Lion,batchsize:32, lr:1e-05\n",
      "step: 1, loss:0.51871634\n",
      "step: 2, loss:0.62306547\n",
      "step: 3, loss:0.56145179\n",
      "step: 4, loss:0.58063710\n",
      "step: 5, loss:0.87819719\n",
      "step: 6, loss:0.68156290\n",
      "step: 7, loss:0.70783985\n",
      "step: 8, loss:0.58677846\n",
      "step: 9, loss:0.74820590\n",
      "step: 10, loss:0.67673910\n",
      "step:10, matthews_corr:0.079514, Acc:69.319271%\n",
      "step: 11, loss:0.61580312\n",
      "step: 12, loss:0.65886658\n",
      "step: 13, loss:0.59500867\n",
      "step: 14, loss:0.61484498\n",
      "step: 15, loss:0.58851606\n",
      "step: 16, loss:0.58159840\n",
      "step: 17, loss:0.61258012\n",
      "step: 18, loss:0.36540309\n",
      "step: 19, loss:0.80590719\n",
      "step: 20, loss:0.52038127\n",
      "step:20, matthews_corr:0.041615, Acc:69.223394%\n",
      "step: 21, loss:0.52342367\n",
      "step: 22, loss:0.65004784\n",
      "step: 23, loss:0.84120882\n",
      "step: 24, loss:0.81308097\n",
      "step: 25, loss:0.57725710\n",
      "step: 26, loss:0.65588945\n",
      "step: 27, loss:0.64778298\n",
      "step: 28, loss:0.69552952\n",
      "step: 29, loss:0.71658319\n",
      "step: 30, loss:0.62821519\n",
      "step:30, matthews_corr:0.288569, Acc:73.250240%\n",
      "step: 31, loss:0.59506452\n",
      "step: 32, loss:0.58211899\n",
      "step: 33, loss:0.57853287\n",
      "step: 34, loss:0.58050972\n",
      "step: 35, loss:0.57560366\n",
      "step: 36, loss:0.69478273\n",
      "step: 37, loss:0.69895053\n",
      "step: 38, loss:0.63174313\n",
      "step: 39, loss:0.56589663\n",
      "step: 40, loss:0.57524419\n",
      "step:40, matthews_corr:0.299728, Acc:73.537872%\n",
      "step: 41, loss:0.63826656\n",
      "step: 42, loss:0.65574998\n",
      "step: 43, loss:0.60966426\n",
      "step: 44, loss:0.65604413\n",
      "step: 45, loss:0.66509920\n",
      "step: 46, loss:0.71462142\n",
      "step: 47, loss:0.49163541\n",
      "step: 48, loss:0.58882123\n",
      "step: 49, loss:0.54819906\n",
      "step: 50, loss:0.57757479\n",
      "step:50, matthews_corr:0.223177, Acc:71.620326%\n",
      "step: 51, loss:0.44697264\n",
      "step: 52, loss:0.88005781\n",
      "step: 53, loss:0.58299929\n",
      "step: 54, loss:0.64065808\n",
      "step: 55, loss:0.59975672\n",
      "step: 56, loss:0.59999222\n",
      "step: 57, loss:0.55958176\n",
      "step: 58, loss:0.58089113\n",
      "step: 59, loss:0.73901731\n",
      "step: 60, loss:0.64028388\n",
      "step:60, matthews_corr:0.447884, Acc:77.660594%\n",
      "step: 61, loss:0.77102876\n",
      "step: 62, loss:0.63447928\n",
      "step: 63, loss:0.55466181\n",
      "step: 64, loss:0.78566766\n",
      "step: 65, loss:0.65390259\n",
      "step: 66, loss:0.60256153\n",
      "step: 67, loss:0.56565201\n",
      "step: 68, loss:0.56478709\n",
      "step: 69, loss:0.43547097\n",
      "step: 70, loss:0.52750242\n",
      "step:70, matthews_corr:0.408208, Acc:76.797699%\n",
      "step: 71, loss:0.43149698\n",
      "step: 72, loss:0.70546710\n",
      "step: 73, loss:0.65988994\n",
      "step: 74, loss:0.61727136\n",
      "step: 75, loss:0.40026289\n",
      "step: 76, loss:0.39739349\n",
      "step: 77, loss:0.46675894\n",
      "step: 78, loss:0.49130282\n",
      "step: 79, loss:0.52603430\n",
      "step: 80, loss:0.69691533\n",
      "step:80, matthews_corr:0.416510, Acc:77.085331%\n",
      "step: 81, loss:0.56352073\n",
      "step: 82, loss:0.58266795\n",
      "step: 83, loss:0.47034863\n",
      "step: 84, loss:0.46204454\n",
      "step: 85, loss:0.38157579\n",
      "step: 86, loss:0.50997007\n",
      "step: 87, loss:0.41529942\n",
      "step: 88, loss:0.68557298\n",
      "step: 89, loss:0.42034930\n",
      "step: 90, loss:0.55186599\n",
      "step:90, matthews_corr:0.486709, Acc:79.482263%\n",
      "step: 91, loss:0.54034525\n",
      "step: 92, loss:0.30661005\n",
      "step: 93, loss:0.65773499\n",
      "step: 94, loss:0.36839062\n",
      "step: 95, loss:0.57143736\n",
      "step: 96, loss:0.28786182\n",
      "step: 97, loss:0.32434580\n",
      "step: 98, loss:0.50704956\n",
      "step: 99, loss:0.53786021\n",
      "step: 100, loss:0.54036838\n",
      "step:100, matthews_corr:0.477611, Acc:79.098754%\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# loss_mat = np.zeros((len(batch_size_list),len(scheduler_list), len(optimizer_list), len(lr_list),steps))\n",
    "\n",
    "report_step = 10 # evaluate test metric each step\n",
    "# metric_mat = np.zeros((len(batch_size_list),len(scheduler_list), len(optimizer_list), len(lr_list),steps//report_step,2))\n",
    "for i,this_batch_size in enumerate(batch_size_list):\n",
    "    for j,this_scheduler in enumerate(scheduler_list):\n",
    "        for k,this_optimizer in enumerate(optimizer_list):\n",
    "            for m, this_lr in enumerate(lr_list):\n",
    "                loss_list = []\n",
    "                metric_list = []\n",
    "                acc_list = []\n",
    "                model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=this_batch_size)\n",
    "                sche, opt = prepare(this_scheduler, this_optimizer)\n",
    "                optimizer = opt(model.parameters(), lr = this_lr)\n",
    "                scheduler = sche(optimizer, num_warmup_steps=int(steps/100),num_training_steps=steps)\n",
    "                step = 0\n",
    "                print(f'Start training for: sche:{this_scheduler},opt:{this_optimizer},batchsize:{this_batch_size}, lr:{this_lr}')\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    for X in train_loader:\n",
    "                        model.train()\n",
    "                        optimizer.zero_grad()\n",
    "                        batch = {k: v.to(device) for k, v in X.items()}\n",
    "                        loss = model(**batch).loss\n",
    "                        print(f\"step: {step+1}, loss:{loss.item():.8f}\")\n",
    "\n",
    "                        # loss_mat[i,j,k,m,step] = loss.item()\n",
    "                        loss_list.append(loss.item())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        step += 1\n",
    "\n",
    "                    # valid\n",
    "                        if step % report_step == 0:\n",
    "                            model.eval()\n",
    "                            with torch.no_grad():\n",
    "                                logits = []\n",
    "                                labelss = []\n",
    "                                for X in test_loader:\n",
    "                                    batch = {k: v.to(device) for k, v in X.items()}\n",
    "                                    logits.append(model(**batch).logits)\n",
    "                                    labelss.append(batch['labels'])\n",
    "                                total_test = torch.concatenate(logits, dim = 0)\n",
    "                                _,predicted = torch.max(total_test,dim = 1)\n",
    "                                real_label =torch.concatenate(labelss,dim=0).cpu().numpy()\n",
    "                                predicted = predicted.cpu().numpy()\n",
    "                                metric = matthews_corrcoef(real_label, predicted)\n",
    "                                acc = np.mean(predicted==real_label)\n",
    "                                # print(i,j,k,m,step//report_step)\n",
    "                                metric_list.append(metric)\n",
    "                                acc_list.append(acc)\n",
    "                                # metric_mat[i,j,k,m,step//report_step - 1,0] = metric\n",
    "                                # metric_mat[i,j,k,m,step//report_step - 1,1] = acc\n",
    "                                print(f\"step:{step}, matthews_corr:{metric:.6f}, Acc:{acc*100:4f}%\")\n",
    "\n",
    "                        if step == steps:\n",
    "                            break\n",
    "                    if step == steps:\n",
    "                      break\n",
    "                file_name = dataset_name+\",batchsize\"+str(this_batch_size)+\",scheduler\"+this_scheduler+\",optimizer\"+str(this_optimizer)+\",LR\"+str(this_lr)\n",
    "                np.save(current_path/(file_name+'loss.npy'),np.array(loss_list))\n",
    "                np.save(current_path/(file_name+'metric.npy'),np.array(metric_list))\n",
    "                np.save(current_path/(file_name+'acc.npy'),np.array(acc_list))\n",
    "\n",
    "                del model\n",
    "                del optimizer\n",
    "                del scheduler\n",
    "                del train_loader\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.5187163352966309,\n 0.6230654716491699,\n 0.56145179271698,\n 0.5806370973587036,\n 0.878197193145752,\n 0.6815629005432129,\n 0.707839846611023,\n 0.586778461933136,\n 0.7482059001922607,\n 0.6767390966415405,\n 0.6158031225204468,\n 0.6588665843009949,\n 0.5950086712837219,\n 0.6148449778556824,\n 0.5885160565376282,\n 0.5815984010696411,\n 0.6125801205635071,\n 0.36540308594703674,\n 0.8059071898460388,\n 0.5203812718391418,\n 0.5234236717224121,\n 0.6500478386878967,\n 0.841208815574646,\n 0.8130809664726257,\n 0.5772570967674255,\n 0.6558894515037537,\n 0.6477829813957214,\n 0.6955295205116272,\n 0.7165831923484802,\n 0.6282151937484741,\n 0.5950645208358765,\n 0.5821189880371094,\n 0.578532874584198,\n 0.5805097222328186,\n 0.5756036639213562,\n 0.6947827339172363,\n 0.6989505290985107,\n 0.6317431330680847,\n 0.5658966302871704,\n 0.5752441883087158,\n 0.6382665634155273,\n 0.6557499766349792,\n 0.609664261341095,\n 0.6560441255569458,\n 0.6650992035865784,\n 0.7146214246749878,\n 0.49163541197776794,\n 0.5888212323188782,\n 0.5481990575790405,\n 0.5775747895240784,\n 0.44697263836860657,\n 0.8800578117370605,\n 0.5829992890357971,\n 0.6406580805778503,\n 0.5997567176818848,\n 0.5999922156333923,\n 0.5595817565917969,\n 0.5808911323547363,\n 0.7390173077583313,\n 0.6402838826179504,\n 0.7710287570953369,\n 0.634479284286499,\n 0.5546618103981018,\n 0.7856676578521729,\n 0.6539025902748108,\n 0.6025615334510803,\n 0.5656520128250122,\n 0.5647870898246765,\n 0.43547096848487854,\n 0.5275024175643921,\n 0.4314969778060913,\n 0.7054671049118042,\n 0.6598899364471436,\n 0.617271363735199,\n 0.40026289224624634,\n 0.39739349484443665,\n 0.46675893664360046,\n 0.49130281805992126,\n 0.5260342955589294,\n 0.696915328502655,\n 0.5635207295417786,\n 0.5826679468154907,\n 0.47034862637519836,\n 0.46204453706741333,\n 0.3815757930278778,\n 0.5099700689315796,\n 0.4152994155883789,\n 0.6855729818344116,\n 0.4203492999076843,\n 0.5518659949302673,\n 0.5403452515602112,\n 0.3066100478172302,\n 0.6577349901199341,\n 0.36839061975479126,\n 0.5714373588562012,\n 0.28786182403564453,\n 0.32434579730033875,\n 0.507049560546875,\n 0.5378602147102356,\n 0.540368378162384]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
